{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Backward Propagation\n",
    "\n",
    "正向传播 (Forward Propagation) 就是通过输入样本和模型数据得到预测值，反向传播 (Backward Propagation) 就是计算损失函数相对于模型参数的梯度，从而对模型进行优化。  \n",
    "\n",
    "[ForwardBackwardPropagation](../Images/backward.png)\n",
    "\n",
    "## PyTorch Implementation\n",
    "\n",
    "首先导入 `torch`，然后初始化一个张量。"
   ],
   "id": "af087b335330d659"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-01-27T14:56:01.705052Z",
     "start_time": "2025-01-27T14:56:01.630385Z"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "x = torch.arange(4.0)\n",
    "print(x)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 2., 3.])\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在计算y关于x的梯度之前，**声明梯度是需要的**。",
   "id": "4e85d82db500950"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:56:01.711805Z",
     "start_time": "2025-01-27T14:56:01.706093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 等价于 x = torch.arange(4, requires_grad=True)\n",
    "# 可以在创建张量时就声明需要梯度\n",
    "x.requires_grad_(True)"
   ],
   "id": "5c685ef69ebf6d18",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.], requires_grad=True)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "定义：$y = 2 x^2 = 2 x^T x$，现在就可以计算$y$的梯度了。注意：**在计算梯度时 (调用 `backward` 方法)，它只能计算一个标量 (scalar) 的梯度**。如果y是一个标量，可以直接使用 `backward` 方法，如果y不是一个标量，而是一个张量或者矩阵，那么就必须先转化为一个标量 (例如使用 `sum` 求和) ，然后再使用 `backward` 方法计算梯度。 ",
   "id": "139b5c5eb2654813"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:56:01.722718Z",
     "start_time": "2025-01-27T14:56:01.713533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y = 2 * torch.dot(x, x)\n",
    "print(y)"
   ],
   "id": "e4ac9ceaa32a2a65",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(28., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "调用反向传播函数 `backward` 来自动计算y关于x在每个样本点上的梯度。",
   "id": "d2acfeaeac49703a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:56:01.735106Z",
     "start_time": "2025-01-27T14:56:01.724637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y.backward()\n",
    "print(x.grad)\n",
    "\n",
    "# 检验y关于x的导数是不是y'= 4x\n",
    "print(x.grad == 4 * x)"
   ],
   "id": "f3cd7518f1760f85",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.,  4.,  8., 12.])\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在默认情况下，PyTorch会累积梯度。在第一次梯度计算中，得到的是正确的结果；在第二次梯度计算中，得到的是第一次的结果加上第二次的结果。PyTorch采用梯度累积的原因是为了支持某些复杂的场景，在这里暂时省略。",
   "id": "372440b07fa8ac18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:56:01.742060Z",
     "start_time": "2025-01-27T14:56:01.736509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.arange(4.0, requires_grad=True)\n",
    "\n",
    "y1 = 2 * torch.dot(x, x)\n",
    "y1.backward()\n",
    "print(f'The first computation of gradient: {x.grad}')\n",
    "\n",
    "y2 = torch.dot(x, x)\n",
    "y2.backward()\n",
    "print(f'The second computation of gradient: {x.grad}')"
   ],
   "id": "e6487833326fc8dd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first computation of gradient: tensor([ 0.,  4.,  8., 12.])\n",
      "The second computation of gradient: tensor([ 0.,  6., 12., 18.])\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果想要避免出现梯度累积的情况，可以使用 `grad.zero_` 方法。这样得到的梯度就是 $y = 2 x^2, y = x^2$ 下分别的梯度。",
   "id": "7befff2478ec84d3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:56:01.751354Z",
     "start_time": "2025-01-27T14:56:01.745649Z"
    }
   },
   "cell_type": "code",
   "source": [
    "y1 = 2 * torch.dot(x, x)\n",
    "y1.backward()\n",
    "print(f'The first computation of gradient: {x.grad}')\n",
    "\n",
    "x.grad.zero_()\n",
    "\n",
    "y2 = torch.dot(x, x)\n",
    "y2.backward()\n",
    "print(f'The second computation of gradient: {x.grad}')\n",
    "\n",
    "x.grad.zero_()"
   ],
   "id": "ac169b3dc8d04856",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first computation of gradient: tensor([ 0., 10., 20., 30.])\n",
      "The second computation of gradient: tensor([0., 2., 4., 6.])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "在PyTorch中，[计算图](#backward-propagation) (Computational Graph) 是自动求导的基础，用于表示张量之间的计算关系和操作顺序。PyTorch的自动求导模块通过计算图来追踪和计算张量的梯度。可以通过 `detach` 方法将，某些计算移到计算图之外。",
   "id": "d35cceccfc81e712"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-27T14:56:01.760152Z",
     "start_time": "2025-01-27T14:56:01.752218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# y是关于x的函数，但是u不是关于x的函数了\n",
    "y = x ** 2\n",
    "u = y.detach()\n",
    "\n",
    "# 如果z关于x求导，得到的就是u\n",
    "z = u * x\n",
    "z.sum().backward()\n",
    "\n",
    "print(x.grad)\n",
    "print(x.grad == u)"
   ],
   "id": "16fefa891c6fcd62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 4., 9.])\n",
      "tensor([True, True, True, True])\n"
     ]
    }
   ],
   "execution_count": 49
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
